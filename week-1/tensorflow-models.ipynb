{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#Linear-Regression-with-TensorFlow\" data-toc-modified-id=\"Linear-Regression-with-TensorFlow-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>Linear Regression with TensorFlow</a></span></li><li><span><a href=\"#TensorFlow-Graphs\" data-toc-modified-id=\"TensorFlow-Graphs-2\"><span class=\"toc-item-num\">2&nbsp;&nbsp;</span>TensorFlow Graphs</a></span></li><li><span><a href=\"#TensorFlow-Sessions\" data-toc-modified-id=\"TensorFlow-Sessions-3\"><span class=\"toc-item-num\">3&nbsp;&nbsp;</span>TensorFlow Sessions</a></span></li><li><span><a href=\"#Using-TensorBoard\" data-toc-modified-id=\"Using-TensorBoard-4\"><span class=\"toc-item-num\">4&nbsp;&nbsp;</span>Using TensorBoard</a></span></li><li><span><a href=\"#Linear-Regression\" data-toc-modified-id=\"Linear-Regression-5\"><span class=\"toc-item-num\">5&nbsp;&nbsp;</span>Linear Regression</a></span><ul class=\"toc-item\"><li><span><a href=\"#Notation\" data-toc-modified-id=\"Notation-5.1\"><span class=\"toc-item-num\">5.1&nbsp;&nbsp;</span>Notation</a></span></li><li><span><a href=\"#Importing-Data\" data-toc-modified-id=\"Importing-Data-5.2\"><span class=\"toc-item-num\">5.2&nbsp;&nbsp;</span>Importing Data</a></span></li><li><span><a href=\"#Constructing-our-graph\" data-toc-modified-id=\"Constructing-our-graph-5.3\"><span class=\"toc-item-num\">5.3&nbsp;&nbsp;</span>Constructing our graph</a></span></li><li><span><a href=\"#Creating-Summaries\" data-toc-modified-id=\"Creating-Summaries-5.4\"><span class=\"toc-item-num\">5.4&nbsp;&nbsp;</span>Creating Summaries</a></span></li><li><span><a href=\"#Training-Session-for-Linear-Regression\" data-toc-modified-id=\"Training-Session-for-Linear-Regression-5.5\"><span class=\"toc-item-num\">5.5&nbsp;&nbsp;</span>Training Session for Linear Regression</a></span></li><li><span><a href=\"#Evaluating-Model-Training-Using-TensorBoard\" data-toc-modified-id=\"Evaluating-Model-Training-Using-TensorBoard-5.6\"><span class=\"toc-item-num\">5.6&nbsp;&nbsp;</span>Evaluating Model Training Using TensorBoard</a></span></li><li><span><a href=\"#Autodiff\" data-toc-modified-id=\"Autodiff-5.7\"><span class=\"toc-item-num\">5.7&nbsp;&nbsp;</span>Autodiff</a></span></li></ul></li><li><span><a href=\"#Extensions\" data-toc-modified-id=\"Extensions-6\"><span class=\"toc-item-num\">6&nbsp;&nbsp;</span>Extensions</a></span></li></ul></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Linear Regression with TensorFlow\n",
    "TensorFlow was built with deep learning in mind, but it is an incredibly flexible framework that can be used for all sorts of computational tasks. To emphasize this point, we will use TensorFlow for linear regression in the problems below. But first, lets start with a simpler computational graph...\n",
    "\n",
    "*Please install the dependencies in the cell below!*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "# sklearn functions for preprocessing\n",
    "from sklearn.datasets import load_diabetes\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import make_pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TensorFlow Graphs\n",
    "We will start off with a small graph as an example. We'll build the graph pictured below:\n",
    "\n",
    "<img src=\"images/computational-graph.png\" alt=\"Computational Graph\" style=\"width: 300px;\"/>\n",
    "\n",
    "Let's begin by making the constant nodes, and the node that adds them together.\n",
    "\n",
    "1. Use `tf.constant` to create a constant that holds the value 5. Save this constant to a variable, `const_5`. Make another constant called `const_2` that holds the value 2.\n",
    "1. When you use `tf.constant`, you can pass a kwarg, `name`, to assign the node a name. Please name `const_5` and `const_2` (use the same name as you use for the python variables!)\n",
    "1. Use the normal addition operator, `+`, to create a node that adds `const_5` and `const_2`. Call this node `add`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "const_5 = ...\n",
    "const_2 = ...\n",
    "add = ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All TensorFlow operations belong to a specific graph. In the cell above, we made three tensorflow operations, but didn't explicitly add them to a graph. TensorFlow creates a default graph which is implicitly used when no other graph is specified. Every Tensorflow operation contains a reference to the graph it belongs to. If your code above is correct, the cell below should pass the assertions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert const_5.graph is tf.get_default_graph()\n",
    "assert const_2.graph is tf.get_default_graph()\n",
    "assert add.graph is tf.get_default_graph()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Constant nodes are used for numbers/tensors that will never change. TensorFlow gives us two main types of nodes for varying values:\n",
    "\n",
    "1. `tf.placeholder`: A node that represents some user provided input. Analogous to the input layer of a neural network. Frequently, placeholders are used to represent training data or test data in a model.\n",
    "1. `tf.Variable`: A node that represents a value that can be reassigned/trained.\n",
    "\n",
    "Below, create a placeholder named `u_input`, and store it to a variable of the same name. When you create a placeholder, you must specify what data type it is meant to hold. Set `dtype=tf.int32`.\n",
    "\n",
    "Then, create another node named `mult`, which multiplies `u_input` and `add`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "u_input = ...\n",
    "mult = ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TensorFlow Sessions\n",
    "\n",
    "Now we've constructed the graph pictured above, but we haven't done any computation! To perform actual computational work in TensorFlow, we have to start a *Session*. Sessions are responsible for dividing computational work across available system resources, like the CPUs and GPUs on your local machine. In the cell below, we will start a new session:\n",
    "\n",
    "* Use `tf.Session` to create a new session, and store this to the variable `sess_1`\n",
    "* Use `sess_1.run` to compute the value of the node `add`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sess_1 = ...\n",
    "# use sess_1.run to compute the value at the node `add`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The value of the node `add` relies on the value of the nodes `const_5` and `const_2`. In the code you wrote above, TensorFlow evaluates only the nodes that add relies on. So, for instance, our node `u_input` was never evaluated, since `add` doesn't rely on its value.\n",
    "\n",
    "Suppose that we want to evaluate the node `mult`. This node *does* rely on the node `u_input`. However, `u_input` doesn't have a value until we give it one ourselves; it is merely a placeholder for our input. In the cell below, we must *feed* the value of `u_input` into our graph.\n",
    "\n",
    "* Create a dict (called `feed_dict`) that stores the values for all of our placeholders. In this case, `feed_dict` should have one key value pair. It's key should be the name TensorFlow has assigned to our `u_input` node (you may access this via `u_input.name`). Let's set `u_input`'s value to 10.\n",
    "* Use `sess_1.run` to evaluate the value of the `mult` node. You may pass `feed_dict` as a kwarg."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "feed_dict = ...\n",
    "# use sess_1.run to evaluate the value of the `mult` node\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We've officially constructed a computational graph and used a session to evaluate nodes in that graph! Now that we are done using this session, we must close the session so that it releases resources it has acquired in order to perform computations.\n",
    "\n",
    "* In the cell below, call `sess_1.close`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# close our tf.Session\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Every TensorFlow session has a TensorFlow graph associated with it. In our example above, we never explicitly declared which graph we wanted `sess_1` to evaluate, so TensorFlow picked the default graph. In the problems below, if we continued to create TensorFlow operations in the global variable environment, we would continue to add nodes to the same graph we've been building.\n",
    "\n",
    "In future problems in this notebook, we will create a completely new graph. When we create a session to evaluate this graph, we must pass this graph explicitly to the Session constructor."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using TensorBoard\n",
    "\n",
    "TensorFlow gives us tools to display information about our graphs and to keep track of node values as sessions progress. These data collection tools can all be found in the `tf.summary` module. The front-end that displays the metadata we collect on our graphs and sessions is called TensorBoard."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`tf.summary.FileWriter` gives us a way to record events that occur during a session. More specifically, an instance of the FileWriter class will write Summary [protocol buffers](https://developers.google.com/protocol-buffers/docs/pythontutorial) to event files. TensorBoard uses the event files created by a writer to display information about TensorFlow sessions.\n",
    "\n",
    "* Store an instance of `tf.summary.FileWriter` in the variable writer.\n",
    "* You must pass it a path to a directory to store event files in. Use the directory `tensorboard/simple-graph`\n",
    "* You should also pass the kwarg `graph`, which specifies the graph we are considering."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "writer = ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A writer is used to save events to disk. However, events don't just happen on their own - we have create nodes in our graph that track events. `tf.summary` has a handful of methods that track different types of data. In this example, we will use `tf.summary.scalar` to track the value of `mult` across many evaluations of `mult`.\n",
    "\n",
    "* In the cell below, create a `tf.summary.scalar` named \"mult_output\", and set it up to track the value of mult. Save this to the variable `mult_summary_op`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "mult_summary_op = ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that `mult_summary_op` is a node/operation that belongs to the graph, just like any other. The result of evaluating this node is a tensor, just like any other node in the graph. However, instead of containing numbers, the tensor produced by `mult_summary_op` will contain a protobuf that describes the state of the `mult` node.\n",
    "\n",
    "Since `mult_summary_op` is a node in our graph, TensorFlow won't evaluate the node unless we explicitly ask it to (or if we ask it to evaluate a node that relies on `mult_summary_op` - it is exceedingly uncommon for TensorFlow operations to rely on summary operations, though).\n",
    "\n",
    "In the cell below, we have created a new session. Instead of creating and closing a session manually, we can use sessions as context managers. This way, the session will automatically close once the `with` block is done executing. Within the `with` block you should:\n",
    "\n",
    "* create a loop that iterates 5000 times\n",
    "* on each iteration, evaluate the nodes `mult` and `mult_summary_op`. You may do this with a single call to `sess.run` by passing it a list of nodes/operations to evaluate.\n",
    "* on each evaluation, you should feed the graph a different value for `u_input`. Try, for example, feeding it the square of the iteration you are on.\n",
    "* save the result of each session run to a variable, `summary`.\n",
    "* use `writer.add_summary` to record the `summary`. Pass it the iteration number you are on as well - this will act as the x-coordinate for a graph of `mult` over each iteration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.Session() as sess:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now you've generated an event file in `tensorboard/simple-graph/run1`! From the command line, run the command:\n",
    "\n",
    "    tensorboard --logdir=tensorboard/simple-graph\n",
    "\n",
    "This will start a server which watches for changes to event files in the `logdir`, and generates/serves a nice visualization of the summary information we've collected. After running the command above, TensorBoard should print a message telling you which port it's listening on - by default it should be port 6006. Visit localhost:< portnum\\>  to see the summaries we've created.\n",
    "\n",
    "The graph tab displays the graph we've created, while the scalar tab should display a line graph of the value of mult on each iteration.\n",
    "\n",
    "In the next section, we will create a linear regression model which collects information on its performance and parameters as it trains. We'll use TensorBoard to display this information. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear Regression\n",
    "Now we will create a linear regression model that learns using gradient descent! Before we proceed, a quick note on notation..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Notation\n",
    "We will denote numpy arrays with variable names beginning with an underscore. Variable names beginning with an alphanumeric character are reserved for tensors in TensorFlow graphs. Lower case variables will be used to denote row or column vectors. For example:\n",
    "\n",
    "* `_X` is a numpy array (2 dimensional or larger)\n",
    "* `X` is a TensorFlow tensor (2 dimensional or larger)\n",
    "* `_y` is a numpy array (with shape (n,) or shape (n, 1))\n",
    "* `y` is a TensorFlow tensor (with shape (n,) or shape (n, 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing Data\n",
    "\n",
    "Run the cell below to import a data set and run some small preprocessing steps. Read through it to see what its doing!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(_X, _y):\n",
    "    pipeline = make_pipeline(StandardScaler()) # additional sklearn preprocessing goes here\n",
    "    _X = pipeline.fit_transform(_X)\n",
    "    _X = np.c_[_X, np.ones(len(_X))] # add column of ones for constant\n",
    "    \n",
    "    # split into train/test sets\n",
    "    _X_train, _X_test, _y_train, _y_test = train_test_split(\n",
    "        _X,\n",
    "        _y,\n",
    "        test_size=.2,\n",
    "        random_state=42\n",
    "    )\n",
    "    \n",
    "    # store y's as column vectors\n",
    "    _y_train = _y_train.reshape(-1,1)\n",
    "    _y_test = _y_test.reshape(-1,1)\n",
    "    \n",
    "    return _X_train, _X_test, _y_train, _y_test\n",
    "\n",
    "_X_train, _X_test, _y_train, _y_test = preprocess(*load_diabetes(return_X_y=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Constructing our graph \n",
    "\n",
    "We want to start work on a whole new graph. Use `tf.Graph` to construct a new graph. Store this variable as `g_linreg`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "g_linreg = ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TensorFlow graphs implement the python context manager interface. By working inside the confines of a `with` block, we may ensure we are creating and adding nodes to the graph we intend to!\n",
    "\n",
    "* Inside the context below, create placeholders for our training data\n",
    "* `X` and `y` should each be placeholders with `dtype=tf.float32`. Name the placeholders accordingly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with g_linreg.as_default():\n",
    "    X = ...\n",
    "    y = ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's use `tf.Variable`, to create a variable `theta`, which stores the parameters of our regression model. Variables are nodes that may be reassigned or trained by other nodes in our graph.\n",
    "\n",
    "* We must pass `tf.Variable` an initial value for the variable.\n",
    "* Use `tf.random_uniform` to initialze theta to a tensor with shape (11,1) that holds random values between -1 and 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with g_linreg.as_default():\n",
    "    theta = ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Linear regression models make predictions the same way a neuron in a neural net calculated its weighted input. That is, we perform the computation:\n",
    "\n",
    "$$\\hat{y} = X\\theta.$$\n",
    "\n",
    "The vector $\\hat{y}$ is the output of the regression model (i.e. the predicted value). Note that we haven't added a bias term here. This is because our preprocessing step added a new column of ones to every observation in our training data. That way, the last parameter in $\\theta$ acts the same way as a bias term would.\n",
    "\n",
    "* Create a new node on our graph called `y_hat` that applies `theta` to `X` using matrix multiplication (Hint: look into `tf.matmul`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with g_linreg.as_default():\n",
    "    y_hat = ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following cell creates nodes in our graph to compute the `error`, `mse`, and `gradient` of the mse function in a similar fashion to how we did this during the precourse."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with g_linreg.as_default():\n",
    "    error = tf.identity(y_hat - y, name=\"error\")\n",
    "    mse = tf.reduce_mean(tf.square(error), name=\"mse\")\n",
    "    gradient = tf.identity(2/353 * tf.matmul(tf.transpose(X), error), name=\"gradient\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At this point, we've created a graph that can apply the parameters `theta` to our training data `X`, and can use the known values `y` to compute the gradient of the mse function. We have all of the pieces in place to perform gradient descent!\n",
    "\n",
    "All we have left to do is reassign the variable `theta` to an improved value! In TensorFlow, assignment is modeled as another node in a graph, just like every other operation. In the cell below:\n",
    "\n",
    "* Use `tf.assign` to create a node that reassigns the value of `theta` to `theta - (.01 * gradient)`. We will name this operation `train`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with g_linreg.as_default():\n",
    "    train = ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating Summaries\n",
    "\n",
    "We're just about ready to train our model - but first lets create summary nodes so we can track our model's progress over time. In the cell below:\n",
    "\n",
    "* create a writer to write event files to `tensorboard/linreg/run1`. Save it to a variable `writer_linreg`.\n",
    "* create a scalar summary that tracks `mse`. Give it an appropriate name. Save it to a variable `scalar_op_linreg`\n",
    "* create a histogram summary that tracks `theta` and name it appropriately. Save it to a variable `hist_op_linreg`.\n",
    "\n",
    "*Remember: summary operations are operations like any other - they must be created on the proper graph. If you aren't careful, you will add them to the default graph, which still refers to our original simple graph from the first section.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# create writer_linreg,  scalar_op_linreg, and hist_op_linreg\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Keeping track of multiple summary operations and evaluating them all individually can become quite a pain. To combat this, TensorFlow provides us with convenient methods to bundle summaries into a single operation.\n",
    "\n",
    "`tf.summary.merge_all` will merge all summary operations in the default graph into a single operation. We could use this as long as we are in a context where `g_linreg` is the default graph. `tf.summary.merge` allows us to explicitly pass a list of summary operations we would like to merge.\n",
    "\n",
    "* use `tf.summary.merge_all` or `tf.summary.merge` to merge `scalar_op_linreg` and `hist_op_linreg`. Save the result to a variable, `summary_op_linreg`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# create summary_op_linreg\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Session for Linear Regression\n",
    "\n",
    "Now we have all the tools necessary to train and document our model! In order to actually perform computation, we have to begin a session. Note that when we create the session, we are passing the graph we created as a kwarg. If we don't do this, TensorFlow will associate the session with the default graph in our current context.\n",
    "\n",
    "After creating our session, we have a number of things to do:\n",
    "1. Whenever we create a `tf.Variable`, we also create a secondary operation whose sole job it is to initialize the variable. Before we use `theta`, we must use `sess.run` to run the initializer for `theta` (which is conveniently referenced by `theta.initializer`)\n",
    "1. Create our `train_feed_dict` and `test_feed_dict` that store our training and test data, respectively. They will be used to pass values to our placeholder nodes.\n",
    "1. Before training our model, lets see how it performs with random values for `theta`. Evaluate `mse`, feeding the model our test data. print the result.\n",
    "1. Let's train our model! Evaluate the `train` and `summary_op_linreg` operations 2000 times.\n",
    "1. Each time, use `writer_linreg` to store the generated summary. Remember to pass the summary and an index representing the epoch the summary is from.\n",
    "1. After training and recording summaries, lets evaluate `mse` on our test data once more. It should have improved greatly!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.Session(graph=g_linreg) as sess:\n",
    "    # 1. initialize theta\n",
    "    # 2. create train_feed_dict and test_feed_dict\n",
    "    # 3. evaluate the mse on our test data, prior to training theta\n",
    "    # 4. train the model for 2000 epochs.\n",
    "    #    remember to evaluate the summary operation each time and write it to memory.\n",
    "    # 5. evaluate the mse on the test data once more.\n",
    "    #    it should be significantly lower after training\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluating Model Training Using TensorBoard\n",
    "\n",
    "Checkout our summary in TensorBoard! What does the model's training curve look like? What is the final distribution of model parameters like? Are there other stats you'd like to record during training?\n",
    "\n",
    "Also notice how messy the visualization of the computational graph is. This was manageable for a very small graph, but now it's hardly helpful because of how many nodes there are. Luckily, there's a fix: with the help of the `tf.name_scope` context manager, we can go from a very messy graph to a much better organized graph:\n",
    "\n",
    "<img src=\"images/clean-vs-messy.png\" alt=\"Messy Graph\" style=\"height: 350px;\"/>\n",
    "\n",
    "If you create an operation named \"an_operation\" within the name_scope \"a_name_scope\", TensorFlow will assign the operation the full name \"a_name_scope/an_operation\". When TensorBoard displays our graph, it will group operations in the same name scope into a single node.\n",
    "\n",
    "* please refactor your code as follows:\n",
    "    1. `X` and `y` belong to the name scope \"input\"\n",
    "    1. `theta` belongs to the name scope \"parameters\"\n",
    "    1. `y_hat` belongs to the name scope \"predictions\"\n",
    "    1. `error` and `mse` belong to the name scope \"loss\"\n",
    "    1. `gradient` and `train` belong to the name scope \"training\"\n",
    "\n",
    "The graph displayed by TensorBoard should be identical to the clean graph in the picture above. You can click on a name scope to expand it, and view individual operations within that name scope."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Autodiff\n",
    "\n",
    "The `gradient` operation in our graph manually computes the gradient of the `mse` operation with respect to the variable `theta`. This is okay for a simple linear regression model. However, computing the gradient of our loss function with respect to model parameters would be a non-trivial problem for larger models like deep neural nets.\n",
    "\n",
    "Luckily, TensorFlow has an autodiff feature which creates operations that calculate these gradients for us! The `tf.gradients` method accepts two main argumets: \n",
    "1. `y`: A tensor, like `mse`, to differentiate\n",
    "1. `xs`: A tensor or list of tensors, like `theta`, which we would like to use for differentiation.\n",
    "\n",
    "`tf.gradients` then returns an operation, or a list of operations, that compute the gradient of `y` with respect to each operation in `xs`. How does this work? Built-in TensorFlow operations implement code to compute their own gradients. When we compose built-in operations to build more complex operations (e.g. how we create the `mse` operation), TensorFlow is able to use well-known rules of differential calculus (e.g. the chain rule) to create a gradient operation by composing the component gradient operations. The algorithm TensorFlow uses to do this is called [reverse-mode automatic differentiation](https://rufflewind.com/2016-12-30/reverse-mode-automatic-differentiation).\n",
    "\n",
    "* Refactor your code so that the `gradients` operation is computed by the `tf.gradients` method"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extensions\n",
    "1. Can you spot the data leak in the preprocessing function?\n",
    "1. Can you build a fully connected neural net using the features of TensorFlow discussed in this notebook?\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  },
  "toc": {
   "nav_menu": {},
   "number_sections": true,
   "sideBar": false,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
