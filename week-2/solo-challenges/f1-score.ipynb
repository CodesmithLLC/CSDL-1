{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#F1-Score\" data-toc-modified-id=\"F1-Score-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>F1 Score</a></span><ul class=\"toc-item\"><li><span><a href=\"#Code-Challenge:-precision\" data-toc-modified-id=\"Code-Challenge:-precision-1.1\"><span class=\"toc-item-num\">1.1&nbsp;&nbsp;</span>Code Challenge: <code>precision</code></a></span></li><li><span><a href=\"#Harmonic-Mean\" data-toc-modified-id=\"Harmonic-Mean-1.2\"><span class=\"toc-item-num\">1.2&nbsp;&nbsp;</span>Harmonic Mean</a></span></li><li><span><a href=\"#Code-Challenge:-f1\" data-toc-modified-id=\"Code-Challenge:-f1-1.3\"><span class=\"toc-item-num\">1.3&nbsp;&nbsp;</span>Code Challenge: <code>f1</code></a></span></li></ul></li><li><span><a href=\"#Extensions\" data-toc-modified-id=\"Extensions-2\"><span class=\"toc-item-num\">2&nbsp;&nbsp;</span>Extensions</a></span></li></ul></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# F1 Score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Today we'll continue learning about statistics used to assess the performance of binary classification models. In particular, we'll compute another summary statistic, called the F1 score, which is derived from the contingency table. Recall that the contingency table is a table holding the following information:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"images/contingency-table.png\" style=\"width:300px;\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The F1 score is an average of the true positive rate (a.k.a. recall) of a classifier and the positive predictive value (a.k.a. precision) of a classifier. Since we've already written a function to assess the true positive rate of a classifier, today we'll start out by writing a function to compute [precision](https://en.wikipedia.org/wiki/Precision_and_recall)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Code Challenge: `precision`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Function Input**\n",
    "\n",
    "1. `y_pred`: An array of predicted classes. This array will hold integers that are either 0 or 1.\n",
    "1. `y`: An array of the actual classes of a data set. Also an array of 0s and 1s."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Function Output**\n",
    "\n",
    "Your function should output the precision of the classifier. This is the percent of observations that were predicted to belong to class 1 that actually belong to class 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def precision(y_pred, y):\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Harmonic Mean"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Usually when we talk about the average value of some numbers, we refer to the arithmetic mean. If we've got a set of numbers $\\{x_i\\}_{i=1}^n$, then their arithmetic mean is $\\frac{1}{n}\\sum_{i=1}^n x_i$. The arithmetic mean is a useful measure of the \"average\" value of a set in many contexts, but it has its flaws. The arithmetic mean is very sensitive to outliers, and isn't a very robust statistic (i.e. it doesn't perform well on data drawn from a large variety of distributions)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The harmonic mean is much less sensitive to outliers, and performs particularly well when we hope to find an average value of rates. For example, if I run for one mile at a rate of 5 mph, and I run another mile at a rate of 10 mph, what was my average speed?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we used the arithmetic mean, we would calculate $\\frac{5 + 10}{2} = 7.5\\ mph$. This is very misleading, though. Let's figure out how we should really calculate my average speed. I've run a total distance of 2 miles, so we'll plan on dividing 2 miles by the number of hours we ran for. This should give us our average speed. It took me $\\frac{1\\ mile}{5\\ mph} = 0.2\\ hours$ to run the first mile. It tooke me $\\frac{1\\ mile}{10\\ mph} = 0.1\\ hours$ to run the second mile. So we calculate my average rate: $\\frac{2\\ miles}{(0.2 + 0.1)\\ hours} \\approx 6.67\\ mph$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We've just computed the harmonic mean of 5 and 10. It turns out, in the case of averaging rates, it makes much more sense to use the harmonic mean. In general, the harmonic mean of a set of numbers $\\{x_i\\}_{i=1}^n$ is calculated: $$\\frac{n}{\\sum_{i=1}^n \\frac{1}{x_i}}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this formula, $n$ is like the number of miles we ran, and each value $\\frac{1}{x_i}$ is how long it took us to run one of those miles."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Code Challenge: `f1`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The F1 score of a binary classifier is the harmonic mean of its precision (positive predictive value) and its recall (true positive rate). Your function should use the `precision` function from earlier. You may also use the `recall` function provided below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def recall(y_pred, y):\n",
    "    n_cp = len(y[y == 1])\n",
    "    n_tp = len(y_pred[(y_pred == 1) & (y == 1)])\n",
    "    return n_tp / n_cp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f1(y_pred, y):\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extensions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Refactor your code so that each function accepts three parameters: `y_pred` (this time an array of floats between 0 and 1), `y` (still an array of integers representing the known class of each observation), and `threshhold` (the classification threshhold above which we consider observations to belong to class 1).\n",
    "1. As the threshhold parameter changes, how do you expect it to affect the value of `precision`?\n",
    "1. What is the relationship between `precision` and `recall`? If we want a model with higher `precision`, what is the effect on `recall`?\n",
    "1. Revisit your `roc.ipynb` solo challenge and complete your ROC curve function and AUC function."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  },
  "toc": {
   "nav_menu": {},
   "number_sections": true,
   "sideBar": false,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
