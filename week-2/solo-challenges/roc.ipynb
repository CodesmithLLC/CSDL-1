{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#ROC-Analysis\" data-toc-modified-id=\"ROC-Analysis-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>ROC Analysis</a></span><ul class=\"toc-item\"><li><span><a href=\"#Code-Challenge:-true_positive_rate\" data-toc-modified-id=\"Code-Challenge:-true_positive_rate-1.1\"><span class=\"toc-item-num\">1.1&nbsp;&nbsp;</span>Code Challenge: <code>true_positive_rate</code></a></span></li><li><span><a href=\"#Code-Challenge:-false_positive_rate\" data-toc-modified-id=\"Code-Challenge:-false_positive_rate-1.2\"><span class=\"toc-item-num\">1.2&nbsp;&nbsp;</span>Code Challenge: <code>false_positive_rate</code></a></span></li><li><span><a href=\"#ROC-Curve\" data-toc-modified-id=\"ROC-Curve-1.3\"><span class=\"toc-item-num\">1.3&nbsp;&nbsp;</span>ROC Curve</a></span></li><li><span><a href=\"#Code-Challenge:-plot_roc\" data-toc-modified-id=\"Code-Challenge:-plot_roc-1.4\"><span class=\"toc-item-num\">1.4&nbsp;&nbsp;</span>Code Challenge: <code>plot_roc</code></a></span></li><li><span><a href=\"#Code-Challenge:-auc\" data-toc-modified-id=\"Code-Challenge:-auc-1.5\"><span class=\"toc-item-num\">1.5&nbsp;&nbsp;</span>Code Challenge: <code>auc</code></a></span></li></ul></li></ul></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ROC Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this challenge, we will learn about Receiver Operating Characteristic (ROC) analysis, and caluculate a commonly used summary statistic for a model called Area Under the ROC Curve (AUC)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Suppose we have a binary classification model that learns to distinguish between two classes, 0 and 1. Our model outputs a value between 0 and 1. Suppose we pick a classification threshhold value of 0.5. If we feed our model an observation and it outputs a value less than 0.5, we will predict the observation belongs to class 0. If the output value is  greater than 0.5, we predict the observation belongs to class 1.\n",
    "\n",
    "In ROC analysis, we will see how our model behavior changes as we alter the threshhold value we use to distinguish between class 0 and class 1."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Code Challenge: `true_positive_rate`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Function Input**\n",
    "1. `y_pred`: A NumPy array of predicted values from our model. Every number in this array will be a float between 0 and 1. The shape of this array will be `(number_of_observations,)`\n",
    "1. `y`: The actual labels for the observations we fed through the model. Every number in this array will be either the integer 0 or the integer 1. It will also have shape `(number_of_observations,)`.\n",
    "1. `threshhold`: A floating point between 0 and 1. We will consider all values in `y_pred` that are less than `threshhold` to belong to class 0 and all values in `y_pred` that are greater than or equal to threshhold to belong to class 1."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Function Output**\n",
    "\n",
    "Your function will output the true positive rate of the model. The true positive rate is the percent of observations that actually belong to class 1 that were correctly predicted to belong to class 1. Refer to the contingency table below for a helpful description of true positive rate."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"images/contingency-table.png\" style=\"width:300px;\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def true_positive_rate(y_pred, y, threshhold):\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Code Challenge: `false_positive_rate`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Function Input**\n",
    "\n",
    "The same input as `true_positive_rate`. See the description under `true_positive_rate` for details.\n",
    "\n",
    "**Function Output**\n",
    "\n",
    "The false positive rate of the model. This is the percent of observations that belong to class 0 that were predicted to belong to class 1. Refer to the contingency table above for a helpful description of the false positive rate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def false_positive_rate(y_pred, y, threshhold):\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ROC Curve"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's consider what happens to the true positive and false positive rates as we slide the classification threshhold paramater between 0 and 1. Take a look at the diagram in the upper right hand side of the image below. The curve on the left is a probability distribution of the value our model will output for all observations that belong to class 0. The curve on the right is a probability distribution of the value our model will output for all observations that belong to class 1. The dividing black line represents the classification threshhold paramater. The area under the left-side distribution that falls to the right of the threshhold is the false positive rate, while the area under the right-side curve that falls to the right of the threshhold is the true positive rate."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"images/ROC_curves.svg.png\" style=\"width:400px\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On one extreme, when the threshhold is very small, we classify all points as belonging to class 1. As a result, our true positive rate is 100% (we properly classify all true members of class 1), but the false positive rate is also 100% (we mistakenly classify all members of class 0 as class 1). On the other extreme, when the threshhold parameter is very large, we end up classifying all points as belonging to class 0. Hence our false positive rate is 0%, but so is our true positive rate."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ideally we'd like a high true positive rate and a low false positive rate. If we set the threshhold at 1 and slowly lower it, both the true positive and false positive rates will rise. Hopefully, the true positive rate grows faster than the false positive rate. As we slide the threshhold parameter, we can plot the false positive rate on the x-axis and the true positive rate on the y-axis. The resulting curve is called the ROC Curve. The lower diagram in the image above is an example of an ROC curve. The dotted line is called the *no-discrimination line*. Let's examine why. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Suppose we have a true positive rate of 15% and a false positive rate of 15%. This means we are classifying 15% of points as belonging to class 1, with no discrimination between points that belong to class 0 and points that belong to class 1. If our model's true positive and false positive rates are equal with value $p_1$, it indicates our model is essentially guessing an observation belongs to class 1 with probability $p_1$, making no discrimination between the observations it is fed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Code Challenge: `plot_roc`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use matplotlib in conjunction with the functions you created above to plot the ROC curve produced by a model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Function Input**\n",
    "\n",
    "1. `y_pred`: A NumPy array of predicted values from our model. Every number in this array will be a float between 0 and 1. The shape of this array will be `(number_of_observations,)`\n",
    "1. `y`: The actual labels for the observations we fed through the model. Every number in this array will be either the integer 0 or the integer 1. It will also have shape `(number_of_observations,)`.\n",
    "1. `steps`: The number of threshhold values to use when creating the ROC Curve.\n",
    "1. `label`: A string that identifies the name of the model we are generating an ROC Curve for."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Your function should produce a plot that looks something like the image below. A test case is provided for you."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](images/example-roc.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot_roc(y_pred, y, steps = 50, label=None):\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = np.load(\"data/y.npy\")\n",
    "y_pred = np.load(\"data/y_pred.npy\")\n",
    "\n",
    "plot_roc(y_pred, y, label=\"binary classifier model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The ROC curve tells us about the discriminatory power of our model - that is, how well is it able to discriminate between members of each class. A number of commonly used summary statistics can be derived from the ROC curve. The most common of these is the Area Under the Curve (AUC) stastic. This statistic is sometimes called AUROC or the c-statistic."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What does the value of AUC tell us about our model? Suppose we choose a random observation from class 0, called $x_0$, and a random observation from class 1, called $x_1$. We feed each of these observations through the model to obtain two values, $p_0$ and $p_1$. The AUC statistic tells us the probability that $p_0 < p_1$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Code Challenge: `auc`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Write a function that computes the AUC statistic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def auc(y_pred, y, steps=50):\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# AUC for y_pred and y should be ~0.77\n",
    "auc(y_pred, y)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  },
  "toc": {
   "nav_menu": {},
   "number_sections": true,
   "sideBar": false,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
